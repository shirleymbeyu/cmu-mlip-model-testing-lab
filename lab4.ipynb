{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fcf777d",
   "metadata": {},
   "source": [
    "You are evaluating a candidate sentiment model to replace a production baseline. Your goal is to determine whether this model should ship.\n",
    "\n",
    "‚ÄúShip‚Äù means: we would choose the candidate model over the baseline for deployment based on the evidence you collect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cfc1c7",
   "metadata": {},
   "source": [
    "### Step 1 - Install the required dependencies, set up W&B and make sure the python version is 3.10 and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14c91f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pyarrow 23.0.0\n",
      "Uninstalling pyarrow-23.0.0:\n",
      "  Successfully uninstalled pyarrow-23.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pyarrow\n",
      "  Using cached pyarrow-23.0.0-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Using cached pyarrow-23.0.0-cp312-cp312-win_amd64.whl (27.7 MB)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-23.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall pyarrow -y\n",
    "%pip install pyarrow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1012c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q wandb datasets transformers evaluate tqdm emoji regex pandas pyarrow scikit-learn nbformat torch\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import wandb\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01639808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from C:\\Users\\STUDENT\\_netrc.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msddaiddo\u001b[0m (\u001b[33msddaiddo-carnegie-mellon-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262b991",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f18b443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.4\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b8e44d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and config:\n",
    "import re, regex, emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "\n",
    "# WANDB CONFIG\n",
    "PROJECT = \"mlip-lab4-slices-2026\"\n",
    "ENTITY = None\n",
    "RUN_NAME = \"baseline_vs_candidate\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3a6beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to compare\n",
    "MODELS = {\n",
    "    \"baseline_model\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    \"candidate_model\":    \"LYTinn/finetuning-sentiment-model-tweet-gpt2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "434c2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label normalization for tweet_eval (0/1/2 -> string labels)\n",
    "ID2LABEL = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "# Many HF sentiment models output labels like LABEL_0 / LABEL_1 / LABEL_2\n",
    "HF_LABEL_MAP = {\"LABEL_0\": \"negative\", \"LABEL_1\": \"neutral\", \"LABEL_2\": \"positive\"}\n",
    "\n",
    "USE_HF_DATASET = True  # set False to use tweets.csv fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289dcac3",
   "metadata": {},
   "source": [
    "### Step 2 - Load a dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9575ae14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user @user what do these '1/2 naked pics' hav...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH: ‚ÄúI had a blue penis while I was this‚Äù [pla...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user That's coming, but I think the vic...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label\n",
       "0  @user @user what do these '1/2 naked pics' hav...  neutral\n",
       "1  OH: ‚ÄúI had a blue penis while I was this‚Äù [pla...  neutral\n",
       "2  @user @user That's coming, but I think the vic...  neutral"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if USE_HF_DATASET:\n",
    "    ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\n",
    "    df = pd.DataFrame(ds[\"test\"]).head(500).copy()\n",
    "    df[\"label\"] = df[\"label\"].map(ID2LABEL)\n",
    "else:\n",
    "    df = pd.read_csv(\"tweets.csv\")\n",
    "    # Ensure it has 'text' and 'label' columns\n",
    "    df = df.rename(columns={c: c.strip() for c in df.columns})\n",
    "    assert {\"text\",\"label\"}.issubset(df.columns), \"tweets.csv must include text,label\"\n",
    "    df[\"label\"] = df[\"label\"].astype(str).str.lower()\n",
    "\n",
    "df = df[[\"text\",\"label\"]].dropna().reset_index(drop=True)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0546f1",
   "metadata": {},
   "source": [
    "### Step 3 - Define Failure-Relevant Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089146d4",
   "metadata": {},
   "source": [
    "#TODO:\n",
    "In this step, you will create **at least 5** metadata columns that help you slice and analyze model behavior in Weights & Biases (W&B).\n",
    "These metadata columns should **capture meaningful properties of the data or model behavior that may influence performance**. You can define them using:\n",
    "\n",
    "1. Value matching (e.g., tweets containing hashtags or mentions)\n",
    "2. Regex patterns (e.g., negation words, strong sentiment terms like love or hate)\n",
    "3. Heuristics (e.g., emoji count, all-caps text, tweet length buckets)\n",
    "\n",
    "Each metadata column should correspond to a potential hypothesis about when or why a model might succeed or fail.\n",
    "These columns will be propagated through inference and included in the final predictions_table logged to W&B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62499536",
   "metadata": {},
   "source": [
    "After inference, your W&B table (df_long) will contain:\n",
    "- The original tweet text\n",
    "- Ground-truth sentiment labels\n",
    "- Model predictions and confidence scores\n",
    "- All metadata columns you defined for slicing\n",
    "\n",
    "You will use these metadata fields in the W&B UI (via the ‚ûï Filter option) to:\n",
    "- Create slices of the data\n",
    "- Compare model behavior across slices\n",
    "- Identify patterns, weaknesses, or regressions that are not visible in overall accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fd5775b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\STUDENT\\AppData\\Local\\Temp\\ipykernel_26816\\2516331889.py:10: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[\"has_negation\"] = df[\"text\"].str.contains(r\"\\b(not|never|no)\\b\", regex=True)\n",
      "C:\\Users\\STUDENT\\AppData\\Local\\Temp\\ipykernel_26816\\2516331889.py:13: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[\"has_contrast\"] = df[\"text\"].str.contains(r\"\\b(but|however|although|yet)\\b\", case=False, regex=True)\n",
      "C:\\Users\\STUDENT\\AppData\\Local\\Temp\\ipykernel_26816\\2516331889.py:14: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df[\"many_exclamations\"] = df[\"text\"].str.contains(r\"(\\!{2,}|\\!\\?)\", regex=True)\n"
     ]
    }
   ],
   "source": [
    "# Step 3 ‚Äì Add slicing metadata (text-only)\n",
    "\n",
    "# Enhanced slicing with hypothesis-driven metadata\n",
    "def count_emojis(text: str) -> int:\n",
    "    return sum(ch in emoji.EMOJI_DATA for ch in str(text))\n",
    "\n",
    "df[\"emoji_count\"] = df[\"text\"].apply(count_emojis).astype(int)\n",
    "df[\"has_hashtag\"] = df[\"text\"].str.contains(r\"#\\w+\", regex=True)\n",
    "df[\"has_mention\"] = df[\"text\"].str.contains(r\"@\\w+\", regex=True)\n",
    "df[\"has_negation\"] = df[\"text\"].str.contains(r\"\\b(not|never|no)\\b\", regex=True)\n",
    "\n",
    "# NEW METADATA\n",
    "df[\"has_contrast\"] = df[\"text\"].str.contains(r\"\\b(but|however|although|yet)\\b\", case=False, regex=True)\n",
    "df[\"many_exclamations\"] = df[\"text\"].str.contains(r\"(\\!{2,}|\\!\\?)\", regex=True)\n",
    "df[\"has_url\"] = df[\"text\"].str.contains(r\"http\\S+\", regex=True)\n",
    "\n",
    "df[\"length_bucket\"] = pd.cut(\n",
    "    df[\"text\"].str.len(),\n",
    "    bins=[0, 50, 100, 200, 1000, 10_000],\n",
    "    labels=[\"0-50\", \"51-100\", \"101-200\", \"201-1000\", \"1001+\"],\n",
    "    include_lowest=True\n",
    ").astype(str)\n",
    "\n",
    "# Updated slice definitions\n",
    "def get_slices(df_any: pd.DataFrame):\n",
    "    return {\n",
    "        \"emoji_gt3\": df_any[\"emoji_count\"] > 3,\n",
    "        \"has_negation\": df_any[\"has_negation\"] == True,\n",
    "        \"has_hashtag\": df_any[\"has_hashtag\"] == True,\n",
    "        \"has_contrast\": df_any[\"has_contrast\"] == True,\n",
    "        \"many_exclamations\": df_any[\"many_exclamations\"] == True,\n",
    "        \"has_url\": df_any[\"has_url\"] == True,\n",
    "        \"has_mention\": df_any[\"has_mention\"] == True,\n",
    "        \"long_tweets\": df_any[\"length_bucket\"].astype(str).isin([\"201-1000\", \"1001+\"]),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3a75f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.6.0+cpu\n",
      "transformers: 4.57.1\n",
      "CUDA available: False\n",
      "Python: c:\\Users\\STUDENT\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# Transformers requires a backend (PyTorch/TensorFlow/Flax). We'll use PyTorch.\n",
    "try:\n",
    "    import torch, transformers, sys\n",
    "    print(\"torch:\", torch.__version__)\n",
    "    print(\"transformers:\", transformers.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"Python:\", sys.executable)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Install PyTorch before proceeding: pip install torch torchvision torchaudio\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ec371",
   "metadata": {},
   "source": [
    "###  Step 4 ‚Äì Run Inference (Two Models)\n",
    "\n",
    "In this step, you'll use two HuggingFace sentiment analysis models to run inference on your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89f69d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41df16e53164094b0e9b99b1f990fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Infer: cardiffnlp/twitter-roberta-base-sentiment-latest:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e79fbb27d74d8496f4196952956cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Infer: LYTinn/finetuning-sentiment-model-tweet-gpt2:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>has_hashtag</th>\n",
       "      <th>has_mention</th>\n",
       "      <th>has_negation</th>\n",
       "      <th>has_contrast</th>\n",
       "      <th>many_exclamations</th>\n",
       "      <th>has_url</th>\n",
       "      <th>length_bucket</th>\n",
       "      <th>model</th>\n",
       "      <th>pred</th>\n",
       "      <th>conf</th>\n",
       "      <th>ex_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user @user what do these '1/2 naked pics' hav...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>51-100</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.804726</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH: ‚ÄúI had a blue penis while I was this‚Äù [pla...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>51-100</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.866949</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user That's coming, but I think the vic...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>51-100</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.763724</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think I may be finally in with the in crowd ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>51-100</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.774047</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@user Wow,first Hugo Chavez and now Fidel Cast...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>101-200</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.416397</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label  emoji_count  \\\n",
       "0  @user @user what do these '1/2 naked pics' hav...   neutral            0   \n",
       "1  OH: ‚ÄúI had a blue penis while I was this‚Äù [pla...   neutral            0   \n",
       "2  @user @user That's coming, but I think the vic...   neutral            0   \n",
       "3  I think I may be finally in with the in crowd ...  positive            0   \n",
       "4  @user Wow,first Hugo Chavez and now Fidel Cast...  negative            0   \n",
       "\n",
       "   has_hashtag  has_mention  has_negation  has_contrast  many_exclamations  \\\n",
       "0        False         True          True         False              False   \n",
       "1        False        False         False         False              False   \n",
       "2        False         True         False          True              False   \n",
       "3         True         True         False         False              False   \n",
       "4        False         True         False         False              False   \n",
       "\n",
       "   has_url length_bucket           model      pred      conf  ex_id  \n",
       "0    False        51-100  baseline_model  negative  0.804726    113  \n",
       "1    False        51-100  baseline_model   neutral  0.866949    363  \n",
       "2    False        51-100  baseline_model   neutral  0.763724    102  \n",
       "3    False        51-100  baseline_model  positive  0.774047    305  \n",
       "4    False       101-200  baseline_model   neutral  0.416397    160  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def run_pipeline(model_id: str, texts: list[str]):\n",
    "    clf = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=model_id,\n",
    "        truncation=True,\n",
    "        max_length=128,     # avoid truncation warnings\n",
    "        framework=\"pt\",\n",
    "        device=-1           # CPU\n",
    "    )\n",
    "    # (Optional) sanity check label mapping for this model\n",
    "    # print(model_id, clf.model.config.id2label)\n",
    "\n",
    "    preds, confs = [], []\n",
    "    for t in tqdm(texts, desc=f\"Infer: {model_id}\"):\n",
    "        out = clf(t)[0]\n",
    "        lbl = HF_LABEL_MAP.get(out[\"label\"], out[\"label\"])\n",
    "        preds.append(lbl)\n",
    "        confs.append(float(out[\"score\"]))\n",
    "    return preds, confs\n",
    "\n",
    "pred_frames = []\n",
    "texts = df[\"text\"].tolist()\n",
    "\n",
    "for model_name, model_id in MODELS.items():\n",
    "    yhat, conf = run_pipeline(model_id, texts)\n",
    "    tmp = df.copy()\n",
    "    tmp[\"model\"] = model_name\n",
    "    tmp[\"pred\"] = yhat\n",
    "    tmp[\"conf\"] = conf\n",
    "    pred_frames.append(tmp)\n",
    "\n",
    "df_long = pd.concat(pred_frames, ignore_index=True)\n",
    "\n",
    "# Add a stable example id so reshaping won't silently drop duplicates\n",
    "df_long[\"ex_id\"] = df_long.groupby([\"text\", \"label\"]).ngroup()\n",
    "\n",
    "df_long.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d189f1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ex_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>has_hashtag</th>\n",
       "      <th>has_mention</th>\n",
       "      <th>has_negation</th>\n",
       "      <th>length_bucket</th>\n",
       "      <th>has_contrast</th>\n",
       "      <th>many_exclamations</th>\n",
       "      <th>has_url</th>\n",
       "      <th>conf_baseline_model</th>\n",
       "      <th>conf_candidate_model</th>\n",
       "      <th>pred_baseline_model</th>\n",
       "      <th>pred_candidate_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Fatty Kim The Third\" üò≠üò≠üò≠</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0-50</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.486252</td>\n",
       "      <td>0.978987</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Focusing on [alt rightists'] respectability.....</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>51-100</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.573571</td>\n",
       "      <td>0.999728</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\"Kim Fatty the Third\"</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0-50</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.849732</td>\n",
       "      <td>0.937710</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"We have lost everything\": Syrians return to r...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>51-100</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.751955</td>\n",
       "      <td>0.994244</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"who's the most wiped out white boy? Zac Efron...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>51-100</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.561232</td>\n",
       "      <td>0.906539</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ex_id                                               text     label  \\\n",
       "0      0                          \"Fatty Kim The Third\" üò≠üò≠üò≠   neutral   \n",
       "1      1  \"Focusing on [alt rightists'] respectability.....   neutral   \n",
       "2      2                              \"Kim Fatty the Third\"  negative   \n",
       "3      3  \"We have lost everything\": Syrians return to r...   neutral   \n",
       "4      4  \"who's the most wiped out white boy? Zac Efron...   neutral   \n",
       "\n",
       "   emoji_count  has_hashtag  has_mention  has_negation length_bucket  \\\n",
       "0            3        False        False         False          0-50   \n",
       "1            0        False        False         False        51-100   \n",
       "2            0        False        False         False          0-50   \n",
       "3            0         True         True         False        51-100   \n",
       "4            0        False        False         False        51-100   \n",
       "\n",
       "   has_contrast  many_exclamations  has_url  conf_baseline_model  \\\n",
       "0         False              False    False             0.486252   \n",
       "1         False              False    False             0.573571   \n",
       "2         False              False    False             0.849732   \n",
       "3         False              False    False             0.751955   \n",
       "4         False              False    False             0.561232   \n",
       "\n",
       "   conf_candidate_model pred_baseline_model pred_candidate_model  \n",
       "0              0.978987             neutral              neutral  \n",
       "1              0.999728            negative              neutral  \n",
       "2              0.937710             neutral              neutral  \n",
       "3              0.994244            negative             positive  \n",
       "4              0.906539             neutral             positive  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4.5 ‚Äì Wide-format Table for Model Comparison (Optional but recommended)\n",
    "# One row per tweet, with baseline + candidate predictions in columns\n",
    "# TODO: Replace with your metadata\n",
    "df_wide = df_long.pivot_table(\n",
    "    index=[\n",
    "        \"ex_id\", \"text\", \"label\",\n",
    "        \"emoji_count\", \"has_hashtag\", \"has_mention\", \"has_negation\", \"length_bucket\", \"has_contrast\", \"many_exclamations\", \"has_url\"\n",
    "    ],\n",
    "    columns=\"model\",\n",
    "    values=[\"pred\", \"conf\"],\n",
    "    aggfunc=\"first\"\n",
    ").reset_index()\n",
    "\n",
    "# Flatten column names (e.g., pred_baseline_model, conf_candidate_model)\n",
    "df_wide.columns = [\"_\".join([c for c in col if c]).strip(\"_\") for col in df_wide.columns]\n",
    "\n",
    "df_wide.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dedaa9",
   "metadata": {},
   "source": [
    "### Step 5: Compute Metrics (Accuracy + Slice Accuracy + Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b77d0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Edit to work for your slices\n",
    "\n",
    "#compute metrics model-wise\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    return accuracy_score(list(y_true), list(y_pred))\n",
    "\n",
    "# Overall accuracy by model (df_long: one row per (tweet, model))\n",
    "overall = df_long.groupby(\"model\").apply(\n",
    "    lambda g: compute_accuracy(g[\"label\"], g[\"pred\"]),\n",
    "    include_groups=False\n",
    ")\n",
    "\n",
    "# Slice accuracy table (uses df_long masks)\n",
    "slice_table = wandb.Table(columns=[\"slice\", \"model\", \"accuracy\"])\n",
    "slice_metrics = {}\n",
    "\n",
    "for slice_name, mask in get_slices(df_long).items():\n",
    "    slice_metrics[slice_name] = {}\n",
    "    for model_name, g in df_long[mask].groupby(\"model\"):\n",
    "        acc = float(compute_accuracy(g[\"label\"], g[\"pred\"]))\n",
    "        slice_table.add_data(slice_name, model_name, acc)\n",
    "        slice_metrics[slice_name][model_name] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "596b544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression rate: 0.408\n",
      "Improvement rate: 0.108\n",
      "Confident regression rate: 0.382\n"
     ]
    }
   ],
   "source": [
    "# TODO: Edit to work for your slices\n",
    "\n",
    "\n",
    "# Regression-aware evaluation (df_eval: one row per tweet, both model outputs) \n",
    "# A regression is when the candidate gets something wrong that the baseline got right.\n",
    "BASELINE = \"baseline_model\"\n",
    "CANDIDATE = \"candidate_model\"\n",
    "\n",
    "# Ensure ex_id exists (safe even if it already exists)\n",
    "df_long = df_long.copy()\n",
    "if \"ex_id\" not in df_long.columns:\n",
    "    df_long[\"ex_id\"] = df_long.groupby([\"text\", \"label\"]).ngroup()\n",
    "\n",
    "# Build df_eval with metadata carried through\n",
    "df_eval = (\n",
    "    df_long.pivot_table(\n",
    "        index=[\n",
    "            \"ex_id\", \"text\", \"label\",\n",
    "            \"emoji_count\", \"has_hashtag\", \"has_mention\", \"has_negation\", \"length_bucket\", \"has_contrast\", \"many_exclamations\", \"has_url\"\n",
    "        ],\n",
    "        columns=\"model\",\n",
    "        values=[\"pred\", \"conf\"],\n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Flatten column names (pred_baseline_model, conf_candidate_model, etc.)\n",
    "df_eval.columns = [\"_\".join([c for c in col if c]).strip(\"_\") for col in df_eval.columns]\n",
    "\n",
    "# Correctness flags\n",
    "df_eval[\"baseline_correct\"]  = df_eval[f\"pred_{BASELINE}\"] == df_eval[\"label\"]\n",
    "df_eval[\"candidate_correct\"] = df_eval[f\"pred_{CANDIDATE}\"] == df_eval[\"label\"]\n",
    "\n",
    "# Regression / improvement flags\n",
    "df_eval[\"regressed\"]   = df_eval[\"baseline_correct\"] & ~df_eval[\"candidate_correct\"]\n",
    "df_eval[\"improved\"]    = ~df_eval[\"baseline_correct\"] & df_eval[\"candidate_correct\"]\n",
    "df_eval[\"both_wrong\"]  = ~df_eval[\"baseline_correct\"] & ~df_eval[\"candidate_correct\"]\n",
    "df_eval[\"both_correct\"]= df_eval[\"baseline_correct\"] & df_eval[\"candidate_correct\"]\n",
    "\n",
    "# Confidence-conditional regression (candidate is confident AND worse than baseline)\n",
    "df_eval[\"confident_regression\"] = df_eval[\"regressed\"] & (df_eval[f\"conf_{CANDIDATE}\"] >= 0.8)\n",
    "\n",
    "# Global regression metrics\n",
    "regression_rate = float(df_eval[\"regressed\"].mean())\n",
    "improvement_rate = float(df_eval[\"improved\"].mean())\n",
    "conf_reg_rate = float(df_eval[\"confident_regression\"].mean())\n",
    "\n",
    "print(\"Regression rate:\", regression_rate)\n",
    "print(\"Improvement rate:\", improvement_rate)\n",
    "print(\"Confident regression rate:\", conf_reg_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12e21e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Edit to work with your slices\n",
    "\n",
    "# Define slices on df_eval (must use columns that exist in df_eval)\n",
    "def get_slices_eval(df_any):\n",
    "    return {\n",
    "        \"emoji_gt3\": df_any[\"emoji_count\"] > 3,\n",
    "        \"has_negation\": df_any[\"has_negation\"] == True,\n",
    "        \"has_hashtag\": df_any[\"has_hashtag\"] == True,\n",
    "        \"has_contrast\": df_any[\"has_contrast\"] == True,\n",
    "        \"many_exclamations\": df_any[\"many_exclamations\"] == True,\n",
    "        \"has_url\": df_any[\"has_url\"] == True,\n",
    "        \"has_mention\": df_any[\"has_mention\"] == True,\n",
    "        \"long_tweets\": df_any[\"length_bucket\"].astype(str).isin([\"201-1000\", \"1001+\"]),\n",
    "    }\n",
    "\n",
    "# Slice-level regression metrics table\n",
    "reg_table = wandb.Table(columns=[\"slice\", \"metric\", \"value\"])\n",
    "reg_metrics = {}\n",
    "\n",
    "for slice_name, mask in get_slices_eval(df_eval).items():\n",
    "    g = df_eval[mask]\n",
    "    if len(g) == 0:\n",
    "        continue\n",
    "\n",
    "    reg = float(g[\"regressed\"].mean())\n",
    "    imp = float(g[\"improved\"].mean())\n",
    "    conf_reg = float(g[\"confident_regression\"].mean())\n",
    "\n",
    "    reg_table.add_data(slice_name, \"regression_rate\", reg)\n",
    "    reg_table.add_data(slice_name, \"improvement_rate\", imp)\n",
    "    reg_table.add_data(slice_name, \"confident_regression_rate\", conf_reg)\n",
    "\n",
    "    reg_metrics[slice_name] = {\n",
    "        \"regression_rate\": reg,\n",
    "        \"improvement_rate\": imp,\n",
    "        \"conf_reg_rate\": conf_reg\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7843a",
   "metadata": {},
   "source": [
    "# Step 6 ‚Äî #TODO: Log to W&B & Analyse Slices\n",
    "# (Make sure PROJECT/ENTITY/RUN_NAME exist from Step 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "425dd4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\STUDENT\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:308: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `list[str]` but got `tuple` - serialized value may not be as expected\n",
      "  Expected `list[str]` but got `tuple` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\STUDENT\\OneDrive - andrew.cmu.edu\\Documents\\GitHub\\cmu-mlip-model-testing-lab\\wandb\\run-20260209_144921-olst1vhf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sddaiddo-carnegie-mellon-university/mlip-lab4-slices-2026/runs/olst1vhf' target=\"_blank\">baseline_vs_candidate_enhanced</a></strong> to <a href='https://wandb.ai/sddaiddo-carnegie-mellon-university/mlip-lab4-slices-2026' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sddaiddo-carnegie-mellon-university/mlip-lab4-slices-2026' target=\"_blank\">https://wandb.ai/sddaiddo-carnegie-mellon-university/mlip-lab4-slices-2026</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sddaiddo-carnegie-mellon-university/mlip-lab4-slices-2026/runs/olst1vhf' target=\"_blank\">https://wandb.ai/sddaiddo-carnegie-mellon-university/mlip-lab4-slices-2026/runs/olst1vhf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B run URL: https://wandb.ai/sddaiddo-carnegie-mellon-university/mlip-lab4-slices-2026/runs/olst1vhf\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>baseline_model_accuracy</td><td>0.698</td></tr><tr><td>candidate_model_accuracy</td><td>0.398</td></tr><tr><td>confident_regression_rate</td><td>0.382</td></tr><tr><td>improvement_rate</td><td>0.108</td></tr><tr><td>regression_rate</td><td>0.408</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">baseline_vs_candidate_enhanced</strong> at: <a href='https://wandb.ai/sddaiddo-carnegie-mellon-university/mlip-lab4-slices-2026/runs/olst1vhf' target=\"_blank\">https://wandb.ai/sddaiddo-carnegie-mellon-university/mlip-lab4-slices-2026/runs/olst1vhf</a><br> View project at: <a href='https://wandb.ai/sddaiddo-carnegie-mellon-university/mlip-lab4-slices-2026' target=\"_blank\">https://wandb.ai/sddaiddo-carnegie-mellon-university/mlip-lab4-slices-2026</a><br>Synced 5 W&B file(s), 4 media file(s), 8 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260209_144921-olst1vhf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6: Log to W&B\n",
    "\n",
    "PROJECT = \"mlip-lab4-slices-2026\"\n",
    "ENTITY = None\n",
    "RUN_NAME = \"baseline_vs_candidate_enhanced\"\n",
    "# Ensure we have a fresh run if re-running\n",
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "    \n",
    "run = wandb.init(project=PROJECT, entity=ENTITY, name=RUN_NAME)\n",
    "\n",
    "# Log tables and metrics\n",
    "wandb.log({\"predictions_table\": wandb.Table(dataframe=df_long)})\n",
    "wandb.log({\"slice_metrics\": slice_table})\n",
    "wandb.log({\"regression_metrics\": reg_table})\n",
    "wandb.log({\"df_eval\": wandb.Table(dataframe=df_eval)})\n",
    "\n",
    "# Log summary metrics\n",
    "for model_name, acc in overall.items():\n",
    "    wandb.summary[f\"{model_name}_accuracy\"] = float(acc)\n",
    "\n",
    "wandb.summary[\"regression_rate\"] = regression_rate\n",
    "wandb.summary[\"improvement_rate\"] = improvement_rate\n",
    "wandb.summary[\"confident_regression_rate\"] = conf_reg_rate\n",
    "\n",
    "print(\"W&B run URL:\", run.get_url())\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c3b74",
   "metadata": {},
   "source": [
    "### Instructions: Exploring Slice-Based Evaluation in W&B\n",
    "\n",
    "# Purpose\n",
    "In this lab, you are evaluating a candidate sentiment model to decide whether it should replace an existing baseline (production) model.\n",
    "You have already:\n",
    "  - run both models on the same dataset\n",
    "  - logged predictions, confidence scores, and metadata to W&B\n",
    "  - created metadata that allows you to slice the data\n",
    "The most important goal is to understand when and why models behave differently.\n",
    "Overall accuracy alone is often misleading.\n",
    "\n",
    "# What to do in W&B\n",
    "1. Open your W&B run\n",
    "  - Click the project link and open the latest run.\n",
    "2. Explore the predictions table\n",
    "  - Go to the Tables tab and open predictions_table.\n",
    "  - Each row is one tweet √ó one model.\n",
    "3. Create and analyze slices (most important)\n",
    "  - Use filters to create meaningful slices \n",
    "    (e.g., negation, emojis, hashtags, long tweets).\n",
    "  - For each slice:\n",
    "    - Compare baseline vs candidate performance.\n",
    "    - Compare slice accuracy to overall accuracy.\n",
    "    - Inspect a few misclassified examples to identify patterns.\n",
    "4. Visualize slice performance\n",
    "  - Open slice_metrics.\n",
    "  - Create bar charts comparing baseline vs candidate accuracy for at least two slices.\n",
    "5. Discuss your findings with the TA\n",
    "  - Explain why slicing reveals issues that overall accuracy hides.\n",
    "  - Say whether the candidate model should be deployed and why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41f83c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mentions: Drastic drop in accuracy (27% vs 70%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negation: Candidate model performs poorly (36%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Contrast: Candidate model handles sentiment fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emojis: Candidate model has 100% accuracy vs 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>URLs: Candidate model performs slightly better...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Insights\n",
       "0  Mentions: Drastic drop in accuracy (27% vs 70%...\n",
       "1  Negation: Candidate model performs poorly (36%...\n",
       "2  Contrast: Candidate model handles sentiment fl...\n",
       "3  Emojis: Candidate model has 100% accuracy vs 0...\n",
       "4  URLs: Candidate model performs slightly better..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saved slice notes\n",
    "saved_slice_notes = [\n",
    "    \"Mentions: Drastic drop in accuracy (27% vs 70%) suggests candidate struggles with conversational context.\",\n",
    "    \"Negation: Candidate model performs poorly (36% vs 57%), likely missing the scope of negation.\",\n",
    "    \"Contrast: Candidate model handles sentiment flips worse than baseline (52% vs 72%).\",\n",
    "    \"Emojis: Candidate model has 100% accuracy vs 0% baseline, suggesting potential over-reliance on surface features.\",\n",
    "    \"URLs: Candidate model performs slightly better (56% vs 50%), showing robustness to external links.\"\n",
    "]\n",
    "pd.DataFrame(saved_slice_notes, columns=[\"Insights\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa906c",
   "metadata": {},
   "source": [
    "### Step 7 - Targeted stress testing with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396331d",
   "metadata": {},
   "source": [
    "TODO: \n",
    "In this step, you will use a Large Language Model (LLM) to generate test cases that specifically target a weakness you observed during slicing.\n",
    "\n",
    "What to do:\n",
    "1. Choose one slice where you noticed poor performance, regressions, or surprising behavior.\n",
    "2. Write a short hypothesis (1‚Äì2 sentences) explaining why the model might struggle on this slice. Example:\n",
    "‚ÄúThe model struggles with tweets that use slang and sarcasm.‚Äù\n",
    "3. Use an LLM to generate 10 test cases designed to test this hypothesis.\n",
    "These can include:\n",
    "    - subtle or ambiguous cases\n",
    "    - difficult or adversarial cases\n",
    "    - small wording changes that affect sentiment\n",
    "4. Re-run both models on the generated test cases (helper script given below.)\n",
    "5. Briefly describe what you observed to the TA:\n",
    "    - Did the same failures appear again?\n",
    "    - notice any new failure patterns?\n",
    "    - would this affect your confidence in deploying the model?\n",
    "\n",
    "Your input can be in the following format:\n",
    "\n",
    "> Examples:\n",
    "> - @user @user That‚Äôs coming, but I think the victims are going to be Medicaid recipients.\n",
    "> - I think I may be finally in with the in crowd #mannequinchallenge  #grads2014 @user\n",
    "> \n",
    "> Generate more tweets using slangs.\n",
    "\n",
    "Use our provided GPTs to start the task: [llm-based-test-case-generator](https://chatgpt.com/g/g-982cylVn2-llm-based-test-case-generator). If you do not have access to GPTs, use the plain ChatGPT or other LLM providers you have access to instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49623362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Paste your 10 generated tweets here:\n",
    "generated_slice_description = [\"Hypothesis: The candidate model struggles with negation\"]\n",
    "\n",
    "generated_cases = [\n",
    "    \"I am not happy with this product.\",\n",
    "    \"The result is not good at all.\",\n",
    "    \"There are no issues with the service.\",\n",
    "    \"I never liked this place.\",\n",
    "    \"Not bad, actually quite decent.\",\n",
    "    \"I don't hate it, but it's okay.\",\n",
    "    \"It wasn't terrible, surprisingly.\",\n",
    "    \"I am not impressed by the quality.\",\n",
    "    \"Nothing went wrong during the process.\",\n",
    "    \"This is no fun.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ce7deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper code to run models on synthetic test cases:\n",
    "\n",
    "def run_on_generated_tests(texts, models=MODELS):\n",
    "    rows = []\n",
    "    for model_name, model_id in models.items():\n",
    "        clf = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=model_id,\n",
    "            truncation=True,\n",
    "            framework=\"pt\",\n",
    "            device=-1\n",
    "        )\n",
    "        for t in texts:\n",
    "            out = clf(t)[0]\n",
    "            rows.append({\n",
    "                \"text\": t,\n",
    "                \"model\": model_name,\n",
    "                \"pred\": HF_LABEL_MAP.get(out[\"label\"], out[\"label\"]),\n",
    "                \"conf\": float(out[\"score\"])\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4a55969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>model</th>\n",
       "      <th>pred</th>\n",
       "      <th>conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am not happy with this product.</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.936887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The result is not good at all.</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.920545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There are no issues with the service.</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.714066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I never liked this place.</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.931402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not bad, actually quite decent.</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.867502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I don't hate it, but it's okay.</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.451993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>It wasn't terrible, surprisingly.</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.746955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I am not impressed by the quality.</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.905853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nothing went wrong during the process.</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.722355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This is no fun.</td>\n",
       "      <td>baseline_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.910570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I am not happy with this product.</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.999493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The result is not good at all.</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.499702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>There are no issues with the service.</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.998205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I never liked this place.</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.998566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Not bad, actually quite decent.</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.999952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I don't hate it, but it's okay.</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.999869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>It wasn't terrible, surprisingly.</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.980636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I am not impressed by the quality.</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.999692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Nothing went wrong during the process.</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.999827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>This is no fun.</td>\n",
       "      <td>candidate_model</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.868725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      text            model      pred  \\\n",
       "0        I am not happy with this product.   baseline_model  negative   \n",
       "1           The result is not good at all.   baseline_model  negative   \n",
       "2    There are no issues with the service.   baseline_model   neutral   \n",
       "3                I never liked this place.   baseline_model  negative   \n",
       "4          Not bad, actually quite decent.   baseline_model  positive   \n",
       "5          I don't hate it, but it's okay.   baseline_model   neutral   \n",
       "6        It wasn't terrible, surprisingly.   baseline_model  positive   \n",
       "7       I am not impressed by the quality.   baseline_model  negative   \n",
       "8   Nothing went wrong during the process.   baseline_model   neutral   \n",
       "9                          This is no fun.   baseline_model  negative   \n",
       "10       I am not happy with this product.  candidate_model  positive   \n",
       "11          The result is not good at all.  candidate_model  positive   \n",
       "12   There are no issues with the service.  candidate_model   neutral   \n",
       "13               I never liked this place.  candidate_model   neutral   \n",
       "14         Not bad, actually quite decent.  candidate_model  negative   \n",
       "15         I don't hate it, but it's okay.  candidate_model   neutral   \n",
       "16       It wasn't terrible, surprisingly.  candidate_model   neutral   \n",
       "17      I am not impressed by the quality.  candidate_model   neutral   \n",
       "18  Nothing went wrong during the process.  candidate_model  negative   \n",
       "19                         This is no fun.  candidate_model   neutral   \n",
       "\n",
       "        conf  \n",
       "0   0.936887  \n",
       "1   0.920545  \n",
       "2   0.714066  \n",
       "3   0.931402  \n",
       "4   0.867502  \n",
       "5   0.451993  \n",
       "6   0.746955  \n",
       "7   0.905853  \n",
       "8   0.722355  \n",
       "9   0.910570  \n",
       "10  0.999493  \n",
       "11  0.499702  \n",
       "12  0.998205  \n",
       "13  0.998566  \n",
       "14  0.999952  \n",
       "15  0.999869  \n",
       "16  0.980636  \n",
       "17  0.999692  \n",
       "18  0.999827  \n",
       "19  0.868725  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_df = run_on_generated_tests(generated_cases)\n",
    "generated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5752e207",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# OPTIONAL: Log synthetic test cases to W&B\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynthetic_tests\u001b[39m\u001b[38;5;124m\"\u001b[39m: wandb\u001b[38;5;241m.\u001b[39mTable(dataframe\u001b[38;5;241m=\u001b[39mgenerated_df)\n\u001b[0;32m      4\u001b[0m })\n",
      "File \u001b[1;32mc:\\Users\\STUDENT\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\lib\\preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreinit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must call wandb.init() before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Log synthetic test cases to W&B\n",
    "wandb.log({\n",
    "    \"synthetic_tests\": wandb.Table(dataframe=generated_df)\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
